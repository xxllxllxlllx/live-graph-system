# ğŸ•¸ï¸ Live Graph System

A powerful web scraping and visualization system with comprehensive Tor integration for exploring both the surface web and dark web (onion sites).

## âœ¨ Features

- **ğŸŒ Web Scraping**: Intelligent crawling with depth control and rate limiting
- **ğŸ§… Tor Integration**: Built-in support for .onion sites via SOCKS5 proxy
- **ğŸ“Š Live Visualization**: Real-time graph updates as data is scraped
- **ğŸ”„ Data Synchronization**: Automatic sync between backend and frontend
- **ğŸ› ï¸ Multiple Scrapers**: Support for TOC, OnionSearch, TorBot, and custom scrapers
- **ğŸ§ª 100% Test Coverage**: Comprehensive test suite with 98 passing tests
- **ğŸ”’ Privacy-Focused**: Respects robots.txt and implements ethical scraping

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Git

### Installation
```bash
# Clone the repository
git clone https://github.com/xxllxllxlllx/live-graph-system.git
cd live-graph-system

# Install dependencies
pip install -r requirements.txt

# For Tor integration (optional)
pip install -r requirements_tor.txt

# Run the system
python main.py
```

### Basic Usage
```bash
# Start the web interface
python -m backend.interfaces.scraper_web_interface

# Access the interface
open http://localhost:5000
```

## ğŸ§… Tor Integration

### Quick Setup
1. **Download Tor Browser**: https://www.torproject.org/download/
2. **Start Tor Browser** (creates proxy on 127.0.0.1:9050)
3. **System automatically detects** and uses Tor proxy

### Supported Onion Sites
- **Ahmia Search**: `juhanurmihxlp77nkq76byazcldy2hlmovfu2epvl5ankdibsot4csyd.onion`
- **BBC News**: `bbcnewsd73hkzno2ini43t4gblxvycyac5aw4gnv7t2rccijh7745uqd.onion`
- **DuckDuckGo**: `duckduckgogg42ts72.onion`
- **Facebook**: `facebookcorewwwi.onion`

### How It Works
The system uses **onion routing** (not VPN):
```
You â†’ Entry Node â†’ Middle Node â†’ Exit Node â†’ Internet
```
For .onion sites, traffic stays within the Tor network for maximum anonymity.

## ğŸ“ New Organized Structure

```
live-graph-system/
â”œâ”€â”€ frontend/                    # Web visualization components
â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â”‚   â””â”€â”€ style.css       # Responsive CSS styling
â”‚   â”‚   â””â”€â”€ js/
â”‚   â”‚       â”œâ”€â”€ d3.v3.min.js    # D3.js library v3 by Mike Bostock
â”‚   â”‚       â””â”€â”€ graph.js        # Enhanced D3.js visualization
â”‚   â””â”€â”€ index.html              # Main graph visualization page
â”‚
â”œâ”€â”€ backend/                     # Python scraper system
â”‚   â”œâ”€â”€ core/                   # Core scraper functionality
â”‚   â”‚   â”œâ”€â”€ web_scraper.py      # Core web scraping engine
â”‚   â”‚   â””â”€â”€ scraper_integration.py # Integration layer for scraper
â”‚   â”œâ”€â”€ interfaces/             # User interfaces
â”‚   â”‚   â”œâ”€â”€ scraper_cli.py      # Command-line interface
â”‚   â”‚   â”œâ”€â”€ scraper_web_interface.py # Flask web interface
â”‚   â”‚   â””â”€â”€ setup_scraper.py    # Setup script for dependencies
â”‚   â”œâ”€â”€ config/                 # Configuration files
â”‚   â”‚   â”œâ”€â”€ scraper_config.py   # Configuration settings
â”‚   â”‚   â””â”€â”€ requirements.txt    # Python dependencies
â”‚   â””â”€â”€ templates/              # Flask templates
â”‚       â””â”€â”€ scraper_interface.html # Web scraper control panel
â”‚
â”œâ”€â”€ data/                       # Data files and logs
â”‚   â”œâ”€â”€ data.json              # Dynamic hierarchical data
â”‚   â”œâ”€â”€ scraper.log            # Scraper log file
â”‚   â””â”€â”€ logs/                  # Additional log files
â”‚
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ README.md              # Original comprehensive documentation
â”‚   â”œâ”€â”€ SYSTEM_STATUS.md       # System status and achievements
â”‚   â””â”€â”€ WEB_SCRAPER_GUIDE.md   # Detailed scraper documentation
â”‚
â”œâ”€â”€ tests/                      # Test files and test websites
â”‚   â”œâ”€â”€ test_websites.py       # Website testing script
â”‚   â””â”€â”€ test-websites/         # Sample test websites
â”‚
â”œâ”€â”€ archive/                    # Legacy files (previously legacy_archive)
â”‚   â””â”€â”€ ...                    # Deprecated random generation files
â”‚
â””â”€â”€ README.md                  # This file - new structure overview
```

## ğŸš€ Quick Start

### Option 1: Web Interface (Recommended)
```bash
# From the backend/interfaces directory
cd backend/interfaces
python setup_scraper.py
python scraper_web_interface.py
# Open: http://localhost:5000
```

### Option 2: Command Line
```bash
# From the backend/interfaces directory
cd backend/interfaces
python scraper_cli.py --url https://example.com --depth 3
```

### Option 3: View Visualization
```bash
# From the frontend directory
cd frontend
python -m http.server 8001
# Open: http://localhost:8001/index.html
```

## ğŸ”§ Key Changes Made

### File Organization
- **Frontend files** moved to `frontend/assets/` with proper subdirectories
- **Python files** organized by function: core, interfaces, config
- **Data files** centralized in `data/` directory
- **Documentation** moved to `docs/` directory
- **Tests** organized in `tests/` directory

### Path Updates
- Updated all import statements in Python files
- Fixed template paths in Flask application
- Updated asset paths in HTML files
- Corrected data.json path in graph.js

### Benefits
- **Cleaner structure**: Logical separation of concerns
- **Easier navigation**: Related files grouped together
- **Better maintainability**: Clear file organization
- **Professional layout**: Industry-standard project structure

## ğŸ“– Documentation

For detailed information about features, usage, and technical implementation, see:
- `docs/README.md` - Comprehensive system documentation
- `docs/SYSTEM_STATUS.md` - Current system status and achievements
- `docs/WEB_SCRAPER_GUIDE.md` - Detailed technical guide

## ğŸ¯ System Features

- **Interactive Tree Graph**: D3.js-powered visualization with smooth animations
- **Hierarchical Web Scraping**: Crawls websites up to 10 depth levels
- **Real-time Updates**: Live data polling and graph updates
- **Multiple Interfaces**: Web control panel and CLI tools
- **Respectful Crawling**: Implements delays and respects robots.txt

---

*Built with â¤ï¸ using D3.js by Mike Bostock, Python, and open-source technologies*
