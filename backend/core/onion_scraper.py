#!/usr/bin/env python3
"""
Onion Web Scraper for Live Graph Visualization System
Crawls .onion sites hierarchically via Tor and generates graph data compatible with D3.js visualization

Built for integration with D3.js-based graph visualization
Uses Tor SOCKS5 proxy for accessing onion sites
Implements the same interface as web_scraper.py but for onion domains
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, urlunparse
import json
import time
import re
import logging
from typing import Dict, List, Set, Optional, Tuple
from dataclasses import dataclass
import threading
from queue import Queue
import os

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('data/onion_scraper.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class OnionScrapingConfig:
    """Configuration for the onion scraper"""
    max_depth: int = 2  # Shallower for onion sites
    max_links_per_page: int = 3  # Fewer links for safety
    request_delay: float = 2.0  # Longer delays for Tor
    timeout: int = 30  # Longer timeout for Tor
    max_retries: int = 2  # Fewer retries
    follow_external_links: bool = False  # Stay within same onion
    user_agent: str = "Mozilla/5.0 (Windows NT 10.0; rv:91.0) Gecko/20100101 Firefox/91.0"  # Tor Browser UA
    tor_proxy_host: str = "127.0.0.1"
    tor_proxy_port: int = 9050

class OnionScraper:
    """Hierarchical onion scraper for graph data generation via Tor"""
    
    def __init__(self, config: OnionScrapingConfig = None):
        self.config = config or OnionScrapingConfig()
        self.visited_urls: Set[str] = set()
        self.is_running = False
        self.total_scraped = 0
        self.current_depth = 0
        
        # Setup Tor proxy session
        self.session = requests.Session()
        self.session.proxies = {
            'http': f'socks5h://{self.config.tor_proxy_host}:{self.config.tor_proxy_port}',
            'https': f'socks5h://{self.config.tor_proxy_host}:{self.config.tor_proxy_port}'
        }
        self.session.headers.update({
            'User-Agent': self.config.user_agent
        })
        
        logger.info(f"OnionScraper initialized with Tor proxy {self.config.tor_proxy_host}:{self.config.tor_proxy_port}")
    
    def check_tor_connection(self) -> bool:
        """Test if Tor proxy is working"""
        try:
            # Test with a known onion site
            test_url = "http://duckduckgogg42ts72.onion"
            response = self.session.get(test_url, timeout=10)
            logger.info("âœ… Tor connection verified")
            return True
        except Exception as e:
            logger.warning(f"âš ï¸ Tor connection failed: {e}")
            return False
    
    def normalize_url(self, url: str) -> str:
        """Normalize URL for consistent comparison"""
        try:
            parsed = urlparse(url)

            # Remove fragment and normalize
            normalized = urlunparse((
                (parsed.scheme or 'http').lower(),
                parsed.netloc.lower(),
                parsed.path.rstrip('/') or '/',
                parsed.params,
                parsed.query,
                ''  # Remove fragment
            ))
            return normalized
        except Exception as e:
            logger.debug(f"URL normalization error for {url}: {e}")
            return url
    
    def is_valid_onion_url(self, url: str) -> bool:
        """Check if URL is a valid onion URL"""
        try:
            parsed = urlparse(url)
            if not parsed.scheme or not parsed.netloc:
                return False

            # Must be .onion domain
            if not parsed.netloc.endswith('.onion'):
                return False

            # For testing, allow simple test domains
            if parsed.netloc in ['test.onion', 'example.onion']:
                return True

            # Check onion address format (v2: 16 chars, v3: 56 chars)
            onion_address = parsed.netloc.replace('.onion', '')
            if len(onion_address) not in [16, 56]:
                # Allow shorter addresses for testing
                if len(onion_address) < 4:
                    return False

            # Basic character validation (allow more characters for testing)
            if not re.match(r'^[a-z0-9]+$', onion_address, re.IGNORECASE):
                return False

            return True
        except Exception as e:
            logger.debug(f"URL validation error for {url}: {e}")
            return False
    
    def extract_title(self, soup: BeautifulSoup, url: str) -> str:
        """Extract page title"""
        try:
            title_tag = soup.find('title')
            if title_tag and title_tag.string:
                title = title_tag.string.strip()
                # Clean up title
                title = re.sub(r'\s+', ' ', title)
                return title[:100]  # Limit length
            
            # Fallback to h1
            h1_tag = soup.find('h1')
            if h1_tag and h1_tag.get_text():
                return h1_tag.get_text().strip()[:100]
            
            # Fallback to domain name
            parsed = urlparse(url)
            return f"Onion Site: {parsed.netloc}"
        
        except Exception as e:
            logger.debug(f"Title extraction error for {url}: {e}")
            return f"Onion Site: {urlparse(url).netloc}"
    
    def extract_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """Extract valid onion links from the page"""
        links = []
        try:
            base_domain = urlparse(base_url).netloc
            
            # Find all anchor tags with href
            for link in soup.find_all('a', href=True):
                href = link['href'].strip()
                if not href or href.startswith('#'):
                    continue
                
                # Convert relative URLs to absolute
                absolute_url = urljoin(base_url, href)
                
                # Validate onion URL
                if not self.is_valid_onion_url(absolute_url):
                    continue
                
                # If not following external links, stay within same domain
                if not self.config.follow_external_links:
                    link_domain = urlparse(absolute_url).netloc
                    if link_domain != base_domain:
                        continue
                
                normalized_url = self.normalize_url(absolute_url)
                if normalized_url not in self.visited_urls:
                    links.append(normalized_url)
                
                # Limit links per page
                if len(links) >= self.config.max_links_per_page:
                    break
            
            logger.debug(f"Extracted {len(links)} valid onion links from {base_url}")
            return links
        
        except Exception as e:
            logger.error(f"Link extraction error for {base_url}: {e}")
            return []
    
    def fetch_onion_page(self, url: str) -> Optional[Tuple[BeautifulSoup, str]]:
        """Fetch and parse an onion page via Tor"""
        for attempt in range(self.config.max_retries):
            try:
                logger.debug(f"Fetching onion page: {url} (attempt {attempt + 1})")
                
                response = self.session.get(
                    url,
                    timeout=self.config.timeout,
                    allow_redirects=True
                )
                
                if response.status_code == 200:
                    # Parse HTML content
                    soup = BeautifulSoup(response.content, 'html.parser')
                    logger.debug(f"Successfully fetched and parsed: {url}")
                    return soup, response.url
                else:
                    logger.warning(f"HTTP {response.status_code} for {url}")
                    
            except requests.exceptions.Timeout:
                logger.warning(f"Timeout fetching {url} (attempt {attempt + 1})")
            except requests.exceptions.ConnectionError as e:
                logger.warning(f"Connection error for {url}: {e}")
            except Exception as e:
                logger.error(f"Unexpected error fetching {url}: {e}")
            
            # Wait before retry
            if attempt < self.config.max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
        
        logger.error(f"Failed to fetch {url} after {self.config.max_retries} attempts")
        return None
    
    def create_error_node(self, url: str, error_message: str) -> Dict:
        """Create an error node for failed requests"""
        return {
            "name": f"Error: {urlparse(url).netloc}",
            "type": "error",
            "description": error_message,
            "url": url,
            "children": []
        }
    
    def scrape_recursive(self, url: str, depth: int = 0) -> Optional[Dict]:
        """Recursively scrape onion site and build hierarchical data"""
        # Check depth and running status
        if depth >= self.config.max_depth or (depth > 0 and not self.is_running):
            return None
        
        normalized_url = self.normalize_url(url)
        if normalized_url in self.visited_urls:
            return None
        
        self.visited_urls.add(normalized_url)
        self.current_depth = max(self.current_depth, depth)
        
        # Fetch the onion page
        result = self.fetch_onion_page(url)
        if not result:
            return self.create_error_node(url, "Failed to fetch onion page")
        
        soup, final_url = result
        self.total_scraped += 1
        
        # Extract page information
        title = self.extract_title(soup, final_url)
        links = self.extract_links(soup, final_url)
        
        # Create node data
        node_type = f"onion_level{depth}" if depth > 0 else "onion_root"
        node_data = {
            "name": title,
            "type": node_type,
            "description": f"Onion URL: {final_url}",
            "url": final_url,
            "onion_site": True,
            "children": []
        }
        
        logger.info(f"ðŸ§… Scraped onion: {title} (depth {depth}, {len(links)} links found)")
        
        # Add delay between requests (important for Tor)
        if self.config.request_delay > 0:
            time.sleep(self.config.request_delay)
        
        # Recursively scrape child links
        for link in links:
            if not self.is_running:
                break
                
            child_node = self.scrape_recursive(link, depth + 1)
            if child_node:
                node_data["children"].append(child_node)
        
        return node_data
    
    def start_scraping(self, root_url: str) -> Dict:
        """Start the onion scraping process"""
        # Validate onion URL first
        if not self.is_valid_onion_url(root_url):
            error_msg = f"Invalid onion URL: {root_url}"
            logger.error(error_msg)
            return self.create_error_node(root_url, error_msg)
        
        # Check Tor connection
        if not self.check_tor_connection():
            error_msg = "Tor connection not available"
            logger.error(error_msg)
            return self.create_error_node(root_url, error_msg)
        
        logger.info(f"ðŸ§… Starting onion scraping: {root_url}")
        self.is_running = True
        self.visited_urls.clear()
        self.total_scraped = 0
        self.current_depth = 0
        
        try:
            result = self.scrape_recursive(root_url)
            if result:
                logger.info(f"ðŸŽ‰ Onion scraping completed: {self.total_scraped} pages, max depth {self.current_depth}")
                return result
            else:
                return self.create_error_node(root_url, "No data scraped")
        
        except Exception as e:
            error_msg = f"Scraping error: {str(e)}"
            logger.error(error_msg)
            return self.create_error_node(root_url, error_msg)
        
        finally:
            self.is_running = False
    
    def stop_scraping(self):
        """Stop the scraping process"""
        self.is_running = False
        logger.info("ðŸ›‘ Onion scraping stopped")
